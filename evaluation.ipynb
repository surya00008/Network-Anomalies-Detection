{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dd86636",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Cell 1: Import Libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import joblib # For loading models and preprocessors\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "# No need to import StandardScaler, OneHotEncoder, ColumnTransformer directly here\n",
    "# as we'll load the pre-fitted preprocessor object which contains these\n",
    "# from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score, roc_curve # Uncomment if you have true labels for quantitative evaluation\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b473fb04",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Cell 2: Load Trained Model, Threshold, and Fitted Preprocessor\n",
    "# --- 2.1 Load Trained Model and Threshold ---\n",
    "model_load_path = 'isolation_forest_model.joblib'\n",
    "threshold_load_path = 'anomaly_threshold.txt'\n",
    "\n",
    "try:\n",
    "    model = joblib.load(model_load_path)\n",
    "    with open(threshold_load_path, 'r') as f:\n",
    "        chosen_threshold = float(f.read())\n",
    "    print(f\"Successfully loaded model from {model_load_path}\")\n",
    "    print(f\"Successfully loaded threshold: {chosen_threshold:.4f}\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: Model or threshold file not found.\")\n",
    "    print(\"Please ensure model_trainer.py ran successfully and the files exist in the current directory.\")\n",
    "    model = None # Set to None to indicate loading failed\n",
    "    chosen_threshold = None\n",
    "    # Subsequent cells will check for None to avoid NameError\n",
    "\n",
    "# --- 2.2 Load the FITTED Preprocessor ---\n",
    "preprocessor_load_path = 'fitted_preprocessor.joblib'\n",
    "try:\n",
    "    preprocessor = joblib.load(preprocessor_load_path)\n",
    "    print(f\"Successfully loaded fitted preprocessor from {preprocessor_load_path}\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: Fitted preprocessor file '{preprocessor_load_path}' not found.\")\n",
    "    print(\"Please ensure preprocessing.ipynb was run and saved the preprocessor.\")\n",
    "    preprocessor = None # Set to None to indicate loading failed\n",
    "except Exception as e:\n",
    "    print(f\"An unexpected error occurred while loading preprocessor: {e}\")\n",
    "    preprocessor = None # Set to None to indicate loading failed\n",
    "\n",
    "# --- 2.3 Define necessary constants used during feature engineering in preprocessing.ipynb ---\n",
    "# These are needed to perform the *same* feature engineering steps on new raw data.\n",
    "common_ports = {\n",
    "    'port_21_ftp': 21, 'port_22_ssh': 22, 'port_23_telnet': 23, 'port_80_http': 80,\n",
    "    'port_443_https': 443, 'port_3389_rdp': 3389, 'port_8080_proxy': 8080\n",
    "}\n",
    "protocol_map = {1: 'ICMP', 6: 'TCP', 17: 'UDP'} # Needed for protocol_name feature\n",
    "\n",
    "# Columns that were dropped *before* the ColumnTransformer in preprocessing.ipynb\n",
    "# This list is used to drop corresponding columns from new data before transformation.\n",
    "features_to_drop_for_preprocessing = [\n",
    "    'timestamp', 'src_ip', 'dst_ip', 'flow_key', 'bidirectional_flow_key',\n",
    "    'sorted_ips', 'sorted_ports', 'protocol' # Include minute_of_hour if you created it in preprocessing.ipynb, otherwise omit\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "397fb275",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Cell 3: Prepare New Data for Evaluation (Using dns.cap)\n",
    "from scapy.all import rdpcap, IP, TCP, UDP # Import scapy here as it's used in this cell\n",
    "\n",
    "# Path to your new, raw PCAP file for evaluation\n",
    "# We are using 'dns.cap' as requested for evaluation.\n",
    "# Ensure 'dns.cap' is in your project directory\n",
    "new_pcap_file_path = 'dns.cap'\n",
    "\n",
    "\n",
    "# Function to parse raw PCAP into extracted features (re-used from pcap_parser.py logic)\n",
    "def parse_raw_pcap_for_eval(pcap_file):\n",
    "    packets = rdpcap(pcap_file)\n",
    "    data = []\n",
    "    for pkt in packets:\n",
    "        if IP in pkt:\n",
    "            ip_src = pkt[IP].src\n",
    "            ip_dst = pkt[IP].dst\n",
    "            protocol = pkt[IP].proto\n",
    "            pkt_len = len(pkt)\n",
    "            timestamp = pkt.time # scapy.time returns a float\n",
    "            src_port = None\n",
    "            dst_port = None\n",
    "            if TCP in pkt:\n",
    "                src_port = pkt[TCP].sport\n",
    "                dst_port = pkt[TCP].dport\n",
    "            elif UDP in pkt:\n",
    "                src_port = pkt[UDP].sport\n",
    "                dst_port = pkt[UDP].dport\n",
    "            row = {\n",
    "                'timestamp': timestamp, 'src_ip': str(ip_src), 'dst_ip': str(ip_dst),\n",
    "                'protocol': protocol, 'src_port': src_port, 'dst_port': dst_port,\n",
    "                'packet_length': pkt_len\n",
    "            }\n",
    "            data.append(row)\n",
    "    return pd.DataFrame(data)\n",
    "\n",
    "# --- Step 3.1: Parse the new raw PCAP into extracted features ---\n",
    "df_new_raw_features = pd.DataFrame() # Initialize as empty DataFrame in case of errors\n",
    "try:\n",
    "    df_new_raw_features = parse_raw_pcap_for_eval(new_pcap_file_path)\n",
    "\n",
    "    # Ensure timestamp is treated as a numeric value before converting to datetime\n",
    "    df_new_raw_features['timestamp'] = pd.to_numeric(df_new_raw_features['timestamp'], errors='coerce')\n",
    "    df_new_raw_features['timestamp'] = pd.to_datetime(df_new_raw_features['timestamp'], unit='s', errors='coerce') # Use errors='coerce' to turn unparseable dates into NaT\n",
    "    df_new_raw_features.dropna(subset=['timestamp'], inplace=True) # Remove rows where timestamp couldn't be parsed\n",
    "\n",
    "    print(f\"\\nSuccessfully parsed new PCAP: {len(df_new_raw_features)} rows.\")\n",
    "    print(df_new_raw_features.head())\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: New PCAP file '{new_pcap_file_path}' not found. Cannot evaluate.\")\n",
    "    print(\"Please ensure the evaluation PCAP file exists in the current directory and the path is correct.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error during new PCAP parsing or initial timestamp conversion: {e}\")\n",
    "\n",
    "# --- Step 3.2: Apply the SAME Feature Engineering as Training Data ---\n",
    "# This block must perfectly replicate the feature engineering steps from preprocessing.ipynb\n",
    "df_new_engineered = df_new_raw_features.copy()\n",
    "\n",
    "if not df_new_engineered.empty:\n",
    "    df_new_engineered['hour_of_day'] = df_new_engineered['timestamp'].dt.hour\n",
    "    df_new_engineered['day_of_week'] = df_new_engineered['timestamp'].dt.dayofweek\n",
    "    df_new_engineered['minute_of_hour'] = df_new_engineered['timestamp'].dt.minute # THIS LINE IS NOW ACTIVE\n",
    "\n",
    "    for feature_name, port_num in common_ports.items():\n",
    "        df_new_engineered[feature_name] = ((df_new_engineered['src_port'] == port_num) | (df_new_engineered['dst_port'] == port_num)).astype(int)\n",
    "\n",
    "    df_new_engineered['protocol_name'] = df_new_engineered['protocol'].map(protocol_map).fillna(df_new_engineered['protocol']).astype(str)\n",
    "\n",
    "    df_new_engineered['sorted_ips'] = df_new_engineered.apply(lambda row: tuple(sorted([row['src_ip'], row['dst_ip']])), axis=1)\n",
    "    df_new_engineered['sorted_ports'] = df_new_engineered.apply(lambda row: tuple(sorted([row['src_port'], row['dst_port']])), axis=1)\n",
    "    df_new_engineered['bidirectional_flow_key'] = df_new_engineered.apply(lambda row:\n",
    "        f\"{row['sorted_ips'][0]}_{row['sorted_ips'][1]}_{row['sorted_ports'][0]}_{row['sorted_ports'][1]}_{row['protocol_name']}\", axis=1)\n",
    "\n",
    "    bidirectional_flow_features_new = df_new_engineered.groupby('bidirectional_flow_key').agg(\n",
    "        bidir_flow_duration=('timestamp', lambda x: (x.max() - x.min()).total_seconds()),\n",
    "        bidir_total_packets=('packet_length', 'size'),\n",
    "        bidir_total_bytes=('packet_length', 'sum'),\n",
    "        bidir_mean_packet_length=('packet_length', 'mean'),\n",
    "        bidir_std_packet_length=('packet_length', 'std'),\n",
    "        num_unique_src_ips=('src_ip', lambda x: x.nunique()),\n",
    "        num_unique_dst_ips=('dst_ip', lambda x: x.nunique())\n",
    "    ).reset_index()\n",
    "\n",
    "    bidirectional_flow_features_new['bidir_flow_duration'] = bidirectional_flow_features_new['bidir_flow_duration'].replace(0, np.nan).fillna(0)\n",
    "    bidirectional_flow_features_new['bidir_std_packet_length'] = bidirectional_flow_features_new['bidir_std_packet_length'].fillna(0)\n",
    "    df_new_engineered = pd.merge(df_new_engineered, bidirectional_flow_features_new, on='bidirectional_flow_key', how='left')\n",
    "\n",
    "    # Retrieve the exact numerical columns from the preprocessor's stored transformers\n",
    "    try:\n",
    "        numerical_cols_from_preprocessor = preprocessor.named_transformers_['num'].get_feature_names_out()\n",
    "    except Exception as e:\n",
    "        print(f\"Error getting numerical column names from preprocessor: {e}. Ensure preprocessor is loaded and correctly configured.\")\n",
    "        numerical_cols_from_preprocessor = []\n",
    "\n",
    "    for col in numerical_cols_from_preprocessor: # Use the list from the fitted preprocessor\n",
    "        if col in df_new_engineered.columns:\n",
    "            df_new_engineered[col] = pd.to_numeric(df_new_engineered[col], errors='coerce').fillna(0)\n",
    "        else:\n",
    "            print(f\"Warning: Numerical column '{col}' from training data is missing in new data. Filling with 0.\")\n",
    "            df_new_engineered[col] = 0.0\n",
    "\n",
    "    # Fill NaN for port columns consistently (as they become strings for OneHotEncoder)\n",
    "    for col in ['src_port', 'dst_port']:\n",
    "        if col in df_new_engineered.columns:\n",
    "            df_new_engineered[col] = df_new_engineered[col].fillna(-1).astype(int).astype(str)\n",
    "        else:\n",
    "            df_new_engineered[col] = '-1'\n",
    "\n",
    "    # Drop the temporary/original columns before transforming using the preprocessor\n",
    "    df_new_data_for_transform = df_new_engineered.drop(columns=features_to_drop_for_preprocessing, errors='ignore')\n",
    "\n",
    "    # --- Step 3.3: Transform the new data using the FITTED preprocessor ---\n",
    "    if preprocessor is not None:\n",
    "        try:\n",
    "            X_new_preprocessed = preprocessor.transform(df_new_data_for_transform)\n",
    "\n",
    "            # Create a DataFrame for the new preprocessed data with correct column names\n",
    "            all_preprocessed_cols = preprocessor.get_feature_names_out()\n",
    "            df_new_preprocessed = pd.DataFrame(X_new_preprocessed, columns=all_preprocessed_cols)\n",
    "            print(\"\\nNew Preprocessed Data Head (ready for inference):\")\n",
    "            print(df_new_preprocessed.head())\n",
    "        except Exception as e:\n",
    "            print(f\"Error during preprocessor transformation: {e}\")\n",
    "            df_new_preprocessed = pd.DataFrame() # Set to empty on error\n",
    "    else:\n",
    "        print(\"Preprocessor not loaded. Cannot transform new data.\")\n",
    "        df_new_preprocessed = pd.DataFrame() # Create empty DataFrame to avoid errors later\n",
    "else:\n",
    "    print(\"New raw features DataFrame is empty. Skipping feature engineering and transformation.\")\n",
    "    df_new_preprocessed = pd.DataFrame() # Create empty DataFrame to avoid errors later\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee8a6afd",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Cell 4: Perform Anomaly Detection Inference on New Data\n",
    "# Check if df_new_preprocessed was successfully created and populated\n",
    "if 'df_new_preprocessed' in locals() and not df_new_preprocessed.empty and model is not None and chosen_threshold is not None:\n",
    "    # Get anomaly scores for the new data\n",
    "    df_new_preprocessed['anomaly_score'] = model.decision_function(df_new_preprocessed.values)\n",
    "\n",
    "    # Apply the loaded threshold to classify anomalies\n",
    "    df_new_preprocessed['is_anomaly_detected'] = (df_new_preprocessed['anomaly_score'] <= chosen_threshold).astype(int)\n",
    "    # Convert 0/1 to 1/-1 for consistency (1: normal, -1: anomaly)\n",
    "    df_new_preprocessed['is_anomaly_detected'] = df_new_preprocessed['is_anomaly_detected'].replace({0: 1, 1: -1})\n",
    "\n",
    "    print(\"\\nAnomaly detection results on new data (first 10 rows):\")\n",
    "    print(df_new_preprocessed[['anomaly_score', 'is_anomaly_detected']].head(10))\n",
    "\n",
    "    num_anomalies_new = df_new_preprocessed[df_new_preprocessed['is_anomaly_detected'] == -1].shape[0]\n",
    "    print(f\"\\nTotal anomalies detected in new data: {num_anomalies_new}\")\n",
    "    print(f\"Proportion of anomalies detected in new data: {num_anomalies_new / len(df_new_preprocessed):.4f}\")\n",
    "else:\n",
    "    print(\"Skipping anomaly detection inference due to empty data, or missing model/threshold, or df_new_preprocessed not defined.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f2c7a01",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Cell 5: Evaluation and Analysis (Qualitative and Quantitative)\n",
    "# --- Qualitative Inspection of Detected Anomalies ---\n",
    "if 'df_new_preprocessed' in locals() and not df_new_preprocessed.empty and 'is_anomaly_detected' in df_new_preprocessed.columns:\n",
    "    detected_anomalies = df_new_preprocessed[df_new_preprocessed['is_anomaly_detected'] == -1]\n",
    "    print(\"\\nCharacteristics of Detected Anomalies (Preprocessed Features):\")\n",
    "    if not detected_anomalies.empty:\n",
    "        print(detected_anomalies.describe()) # Summary statistics for preprocessed features of anomalies\n",
    "\n",
    "        # To see original context (IPs, ports etc.), you would merge with df_new_raw_features\n",
    "        # Ensure df_new_raw_features is populated from Cell 3.\n",
    "        # This merge uses the index to align.\n",
    "        if 'df_new_raw_features' in locals() and not df_new_raw_features.empty:\n",
    "            df_anomalies_with_context = df_new_raw_features.iloc[detected_anomalies.index].copy()\n",
    "            df_anomalies_with_context['anomaly_score'] = detected_anomalies['anomaly_score']\n",
    "            df_anomalies_with_context['is_anomaly_detected'] = detected_anomalies['is_anomaly_detected']\n",
    "            print(\"\\nOriginal context of Detected Anomalies (first 5):\")\n",
    "            print(df_anomalies_with_context.head())\n",
    "        else:\n",
    "            print(\"\\nOriginal raw features not available for context display.\")\n",
    "\n",
    "    else:\n",
    "        print(\"No anomalies detected in the new data based on the current threshold.\")\n",
    "else:\n",
    "    print(\"Cannot perform anomaly analysis: Data or anomaly detection results are missing.\")\n",
    "\n",
    "# --- Visualize Anomaly Scores (New Data) ---\n",
    "if 'df_new_preprocessed' in locals() and not df_new_preprocessed.empty and 'anomaly_score' in df_new_preprocessed.columns:\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    sns.histplot(df_new_preprocessed['anomaly_score'], bins=50, kde=True, color='purple')\n",
    "    plt.title('Distribution of Anomaly Scores on New (Evaluation) Data', fontsize=16)\n",
    "    plt.xlabel('Anomaly Score', fontsize=12)\n",
    "    plt.ylabel('Frequency', fontsize=12)\n",
    "    if chosen_threshold is not None:\n",
    "        plt.axvline(x=chosen_threshold, color='red', linestyle='--', label=f'Chosen Threshold: {chosen_threshold:.4f}')\n",
    "    plt.legend(fontsize=10)\n",
    "    plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"Cannot plot anomaly score distribution: Data or anomaly scores are missing.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
